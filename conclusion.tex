\section{Conclusion}
This work presents a performance model of MapReduce Applications with Apache Hadoop, which is useful for making an optimal configuration of Hadoop for improving the application performance.
Different from existing work, we take into account the parallelism between the adjacent processing phases.
With a clear understanding of the MapReduce implementation by reading source code of Hadoop, we give an anatomy of MapReduce job processing. It shows which specific phases a MapReduce job must go through and which adjacent phases are run in parallel for performance consideration.
Then we come up with the performance models for Map and Reduce respectively, where we particularly address the issue of predicting the execution time when two execution phases overlap with each other.
As some parameters in the performance models rely on the characteristics of applications themselves and the specific underlying resources, we design a light-weight instrumentation tool, LTrace, to capture the application and resource specific parameters by running an application with LTrace for only one time.
We have conducted a set of experiments with HiBench workload including WordCount, TeraSort and PageRank jobs. Experimental results with different size of data demonstrate that our model can achieve a prediction accuracy as high as over 94\%, which greatly outperforms the comparing approach.

In future, we will continue to work on how to take advantage of our performance models to generate an optimal configuration for MapReduce applications. Beyond that, though our model works well for the batch jobs, we believe it is worthwhile to consider the commonly used query jobs.  The processing of query jobs depends on the specific query constraints provided by users, which usually involves complex data operations and many map/reduce tasks. This can pose new challenges for the generality of the performance model. Our future work will concern this issue by incorporating other benchmarks such as TPC-DS\cite{Nambiar2006The} and BigBench~\cite{Ghazal2013BigBench}.
