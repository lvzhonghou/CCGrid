\section{Introduction}
In recent years, with the rapid development of e-commerce, search engines, social networks and mobile internet,
the challenge of data processing has been growing rapidly due to the fast-growing volumes of data.
MapReduce~\cite{Dean2004MapReduce} is one of the most popular and efficient big data computing framework.
Hadoop~\cite{White2010Hadoop} is the most frequently-used open source implementation of MapReduce. 
Programmers, researchers and even some computer science beginners can deal with big data processing jobs, such as log analysis, index building and data mining, on the platform of hadoop.

A MapReduce Job consists of three parts: user-defined programs, input data and configuration parameters. Configuration parameters are the user-specified set of options. The selection of configuration parameters has a significant impact on the execution overhead of a MapReduce job, and the user can optimize the parameters to speedup job processing~\cite{Herodotou2011Profiling, Shi2014MRTuner, Li2014MRONLINE}. However, this is a great challenge for the IT professionals, even for people who have a good understanding of Hadoop's internal mechanism, to select the most effective set of parameters. To optimize the configuration parameters more effectively, a productive tool is necessary for users to predict MapReduce job's execution time with varying configuration parameters.

Nowadays, there are some related research~\cite{Herodotou2011Profiling, Shi2014MRTuner, Herodotou2011Hadoop} on the prediction of MapReduce job's execution time, which can be applied to optimize the configuration parameters ~\cite{Herodotou2011No}.  The construction of job's performance model is one of the most effective methods to forecast the execution time of MapReduce jobs. However, there are some deficiencies in the existing performance models. The present models are constructed to predict the executive cost of each individual phase of MapReduce, and the simple addition of each phase's cost is the prediction of the execution time. As a matter of fact, the executing procedure of a MapReduce job in Hadoop system is complicated, which is not computed step by step purely. For example, multi-threading technology leads to the parallel execution between multiple individual phases. 

 To construct an accurate performance model for the MapReduce jobs, it is necessary to confirm whether the parallelism exists between multiple phases, and to measure the job's execution time in the presence of parallelism. We have analyzed the practical execution process of Hadoop MapReduce through reading its source code. As a result, we find that some fine-grained phases' execution parallels with others'. Furthermore, we have conducted a thorough analysis of the associated factors that affect the execution time of each fine-grained phase and the overlap time between multiple phases.

In addition, we need an extra lightweight monitoring and analysis tool to obtain the execution statistical data of MapReduce Jobs, and these statistics contain the statistical information about the dataflow and execution time of each fine-grained phase. The recent related works use the third-party profilers to capture the executive statistics, and this profiler increases the overhead by 5\%-30\%, which negatively affects the execution performance of the running MapReduce Jobs and consequently brings errors to the accuracy of statistics. To relieve the negative impact of the profiler's heavy overhead, we design a lightweight approach, which is based on the log printing system, to collect the statistics. The row data of the running jobs' execution information is printed to the log files through the log message firstly, and then the statistics are obtained off-line through the extraction of the log files. This approach separates the statistics collection into two phases, which minimize the negative impact of the profiler's overhead.

This paper introduces how to predict the execution time through the construction of the MapReduce performance model, the main contributions of this paper are as follows:
\begin{enumerate}
    \item  we construct a MapReduce performance model MR. Prophet, which can predict the execution time of a MapReduce job with high accuracy.
    \item  we design a lightweight log-based approach to capture the execution statistics of MapReduce jobs.
    \item  we have conducted extensive experiments with HiBench, and verified the accuracy of Mr. Prophet through the comparison with the related work.
\end{enumerate}

The rest of this paper is organized as follows. Section 2 presents a fine anatomy of the processing of MapReduce jobs. Section 3 introduces the design and implementation of the lightweight profiler, LTrace. Section 4 describes a new Hadoop MapReduce's performance model. Section 5 presents the evaluation results with HiBench workload. And Section 6 introduces some related work, finally Section 7 concludes this paper.
